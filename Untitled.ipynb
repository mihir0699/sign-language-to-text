{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd68c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_path = r'C:\\Users\\mihir\\OneDrive\\Desktop\\Final Year Project\\train'\n",
    "test_path = r'C:\\Users\\mihir\\OneDrive\\Desktop\\Final Year Project\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0baa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e3aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 602 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "# test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8202d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd95634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "# from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5f0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(2,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b803343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68defc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.9518WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 11s 147ms/step - loss: 0.6031 - accuracy: 0.9518 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9950WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 65ms/step - loss: 0.0484 - accuracy: 0.9950 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9934WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 64ms/step - loss: 0.0199 - accuracy: 0.9934 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9950WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 62ms/step - loss: 0.0064 - accuracy: 0.9950 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9967WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 0.0047 - accuracy: 0.9967 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9983WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 5s 76ms/step - loss: 0.0180 - accuracy: 0.9983 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9917WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 72ms/step - loss: 0.0159 - accuracy: 0.9917 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9967WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 72ms/step - loss: 0.0037 - accuracy: 0.9967 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9967WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 4s 72ms/step - loss: 0.0050 - accuracy: 0.9967 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "61/61 [==============================] - 5s 74ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de904841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_model_dataflair3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc5a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
